import json
import os
import torch
import numpy as np
from tqdm import tqdm
from datasets import load_dataset
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import re
from collections import Counter

# SymPyé–¢é€£ (æ­£è§£åˆ¤å®šç”¨) - Phase 2ã¨åŒã˜å¼·åŠ›ãªã‚‚ã®ã‚’ä½¿ç”¨
from latex2sympy2 import latex2sympy
from sympy import simplify

# ==========================================
# 1. è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
# ==========================================
# ãƒ¢ãƒ‡ãƒ«
POLICY_MODEL_ID = "Qwen/Qwen2.5-Math-7B-Instruct"   # ç”Ÿæˆç”¨
PRM_MODEL_PATH = "models/delta_prm_1.5b_pre_v1"            # è©•ä¾¡ç”¨

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
DATASET_NAME = "HuggingFaceH4/MATH-500" # MATHãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®ä»£è¡¨çš„ãª500å•ã‚µãƒ–ã‚»ãƒƒãƒˆ

# å®Ÿé¨“è¨­å®š
N_SAMPLES = 16          # Best-of-N (16å€‹ç”Ÿæˆ)
MAX_TOKENS = 2048
TEMPERATURE = 0.7

# PRMè¨­å®š
PRM_BATCH_SIZE = 8
PRM_MAX_LENGTH = 3072   # å­¦ç¿’æ™‚ã¨åŒã˜é•·ã•ã‚’ç¢ºä¿
STEP_MERGE_CHARS = 50   # å­¦ç¿’æ™‚ã¨åŒã˜ãƒãƒ¼ã‚¸åŸºæº–

# ==========================================
# 2. æ•°å­¦çš„æ­£è§£åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ (Phase 2ã‹ã‚‰ç§»æ¤)
# ==========================================

def extract_answer_content(text):
    """\boxed{} ã®ä¸­èº«ã‚’æŠ½å‡º"""
    if not text: return None
    # æœ€å¾Œã®boxedã‚’æŠ½å‡º
    matches = re.findall(r"\\boxed\{(.*?)\}", text)
    if matches: return matches[-1].strip()
    return None

def robust_float_check(pred, gold):
    try:
        def to_float(s):
            s = str(s).replace(r"\frac", "").replace("{", "(").replace("}", ")").replace("^", "**")
            s = s.replace(r"\left", "").replace(r"\right", "").replace(",", "")
            return float(eval(s))
        if not any(c.isalpha() for c in str(pred)) and not any(c.isalpha() for c in str(gold)):
            return abs(to_float(pred) - to_float(gold)) < 1e-6
    except:
        pass
    return False

def check_correctness(pred_str, gold_str):
    """
    äºˆæ¸¬ã¨æ­£è§£ãŒæ•°å­¦çš„ã«ç­‰ã—ã„ã‹åˆ¤å®šã™ã‚‹ (SymPy + æ•°å€¤ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—)
    """
    if not pred_str or not gold_str: return False
    pred_str = str(pred_str).strip()
    gold_str = str(gold_str).strip()
    
    if pred_str == gold_str: return True

    try:
        # latex2sympy ã§ãƒ‘ãƒ¼ã‚¹ã—ã¦æ¯”è¼ƒ
        sym_pred = latex2sympy(pred_str)
        sym_gold = latex2sympy(gold_str)
        if simplify(sym_pred - sym_gold) == 0:
            return True
    except Exception:
        # å¤±æ•—ã—ãŸã‚‰æ•°å€¤æ¯”è¼ƒã¸
        return robust_float_check(pred_str, gold_str)

    return False

def reduce_step_count(steps, min_chars=50):
    """å­¦ç¿’æ™‚ã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ã§ã‚¹ãƒ†ãƒƒãƒ—çµåˆ"""
    merged = []
    buf = ""
    for s in steps:
        if not buf: buf = s; continue
        if len(s) < min_chars or len(buf) < min_chars: buf += "\n" + s
        else: merged.append(buf); buf = s
    if buf: merged.append(buf)
    return merged

# ==========================================
# 3. è©•ä¾¡ã‚¯ãƒ©ã‚¹
# ==========================================

class Evaluator:
    def __init__(self):
        print(f"Loading {DATASET_NAME}...")
        # MATH-500ã¯ 'problem', 'solution', 'answer' ã‚«ãƒ©ãƒ ã‚’æŒã¤
        self.dataset = load_dataset(DATASET_NAME, split="test")
        print(f"Target problems: {len(self.dataset)}")

    def run_generation(self):
        """vLLMã§å›ç­”ç”Ÿæˆ (Policy)"""
        print(f"Initializing Policy Model ({POLICY_MODEL_ID})...")
        llm = LLM(
            model=POLICY_MODEL_ID,
            tensor_parallel_size=torch.cuda.device_count(),
            trust_remote_code=True,
            gpu_memory_utilization=0.8,
            dtype="bfloat16"
        )
        tokenizer = AutoTokenizer.from_pretrained(POLICY_MODEL_ID)
        
        prompts = []
        raw_data = []
        
        system_prompt = "Please reason step by step and put your final answer within \\boxed{}."
        
        print("Preparing prompts...")
        for item in self.dataset:
            question = item["problem"]
            
            # MATH-500ã®æ­£è§£ãƒ‡ãƒ¼ã‚¿å‡¦ç†
            # 'answer' ã‚«ãƒ©ãƒ ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ã†ã€ãªã‘ã‚Œã° 'solution' ã‹ã‚‰æŠ½å‡º
            if "answer" in item and item["answer"]:
                gold = item["answer"]
            else:
                gold = extract_answer_content(item["solution"])

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question}
            ]
            prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            
            prompts.append(prompt)
            raw_data.append({"question": question, "gold": gold})

        # ç”Ÿæˆå®Ÿè¡Œ
        print(f"Generating {N_SAMPLES} paths per problem...")
        params = SamplingParams(n=N_SAMPLES, temperature=TEMPERATURE, max_tokens=MAX_TOKENS)
        outputs = llm.generate(prompts, params)
        
        results = []
        for i, output in enumerate(outputs):
            paths = [o.text for o in output.outputs]
            results.append({
                "problem": raw_data[i]["question"],
                "gold": raw_data[i]["gold"],
                "paths": paths
            })
            
        # ãƒ¡ãƒ¢ãƒªè§£æ”¾
        from vllm.distributed.parallel_state import destroy_model_parallel
        destroy_model_parallel()
        del llm
        torch.cuda.empty_cache()
        print("Generation finished. Released vLLM memory.")
        
        return results

    def run_scoring(self, generated_results):
        """PRMã«ã‚ˆã‚‹ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°"""
        print(f"Initializing PRM Model ({PRM_MODEL_PATH})...")
        prm_tokenizer = AutoTokenizer.from_pretrained(PRM_MODEL_PATH)
        prm_model = AutoModelForSequenceClassification.from_pretrained(
            PRM_MODEL_PATH, 
            num_labels=1, 
            torch_dtype=torch.bfloat16,
            device_map="auto"
        )
        prm_model.eval()
        
        scored_results = []
        
        print("Scoring paths...")
        for item in tqdm(generated_results, desc="PRM Scoring"):
            problem = item["problem"]
            paths = item["paths"]
            path_scores = []
            
            for path in paths:
                # 1. ã‚¹ãƒ†ãƒƒãƒ—åˆ†å‰²ã¨çµåˆ (å­¦ç¿’æ™‚ã¨åŒã˜å‰å‡¦ç†)
                raw_steps = [s.strip() for s in re.split(r'\n\s*\n', path) if s.strip()]
                if not raw_steps: 
                    raw_steps = [s.strip() for s in path.split('\n') if s.strip()]
                
                steps = reduce_step_count(raw_steps, min_chars=STEP_MERGE_CHARS)
                
                if not steps:
                    path_scores.append(-99.0)
                    continue
                
                # 2. å…¨ã‚¹ãƒ†ãƒƒãƒ—ã‚’è©•ä¾¡ã—ã¦æœ€å°å€¤(Min)ã‚’å–ã‚‹
                # å…¥åŠ›ä½œæˆ: [Problem + Step1], [Problem + Step1 + Step2]...
                step_inputs = []
                curr_text = problem
                for step in steps:
                    curr_text += "\n" + step
                    step_inputs.append(curr_text)
                
                step_rewards = []
                with torch.no_grad():
                    # ãƒãƒƒãƒæ¨è«–
                    for i in range(0, len(step_inputs), PRM_BATCH_SIZE):
                        batch = step_inputs[i : i+PRM_BATCH_SIZE]
                        inputs = prm_tokenizer(
                            batch, 
                            return_tensors="pt", 
                            padding=True, 
                            truncation=True, 
                            max_length=PRM_MAX_LENGTH
                        ).to(prm_model.device)
                        
                        out = prm_model(**inputs)
                        step_rewards.extend(out.logits.squeeze(-1).tolist())
                
                # ãƒ‘ã‚¹ã‚¹ã‚³ã‚¢ = Min(ã‚¹ãƒ†ãƒƒãƒ—å ±é…¬)
                # ã©ã‚“ãªã«è‰¯ãã¦ã‚‚ä¸€åº¦ã§ã‚‚è‡´å‘½çš„ãªãƒŸã‚¹(ä½ã„å€¤)ãŒã‚ã‚Œã°ä½è©•ä¾¡ã«ã™ã‚‹
                final_score = min(step_rewards) if step_rewards else -99.0
                path_scores.append(final_score)
            
            item["scores"] = path_scores
            scored_results.append(item)
            
        return scored_results

    def calculate_metrics(self, results):
        """3ã¤ã®æŒ‡æ¨™ã‚’è¨ˆç®—ã—ã¦æ¯”è¼ƒ"""
        print("Calculating metrics...")
        
        # ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
        pass1_total_correct = 0  # ç”Ÿæˆã•ã‚ŒãŸå…¨ãƒ‘ã‚¹ã®ã†ã¡æ­£è§£ã ã£ãŸæ•° (å¹³å‡è¨ˆç®—ç”¨)
        total_generated_paths = 0
        
        maj_correct_count = 0    # å¤šæ•°æ±ºã§æ­£è§£ã—ãŸå•é¡Œæ•°
        prm_correct_count = 0    # PRMã§æ­£è§£ã—ãŸå•é¡Œæ•°
        total_problems = len(results)
        
        for item in tqdm(results, desc="Checking Correctness"):
            gold = item["gold"]
            paths = item["paths"]
            scores = item["scores"]
            
            # å„ãƒ‘ã‚¹ã‹ã‚‰ç­”ãˆã‚’æŠ½å‡º
            extracted_answers = [extract_answer_content(p) for p in paths]
            
            # --- 1. Pass@1 (Average Accuracy) ---
            # ç”Ÿæˆã•ã‚ŒãŸNå€‹ã®ãƒ‘ã‚¹ãã‚Œãã‚Œã®æ­£èª¤ã‚’åˆ¤å®š
            path_correctness = []
            valid_answers_for_voting = []
            
            for ans in extracted_answers:
                is_correct = check_correctness(ans, gold)
                path_correctness.append(is_correct)
                if ans: valid_answers_for_voting.append(ans)
            
            pass1_total_correct += sum(path_correctness)
            total_generated_paths += len(paths)
            
            # --- 2. Majority Voting ---
            if valid_answers_for_voting:
                # å˜ç´”ãªæ–‡å­—åˆ—ä¸€è‡´ã§ã®å¤šæ•°æ±º (è¡¨è¨˜æºã‚Œã¯SymPyã§å¸åã§ããªã„ãŸã‚æ–‡å­—åˆ—ãƒ™ãƒ¼ã‚¹ãŒä¸€èˆ¬çš„)
                # ãŸã ã—ã€å³å¯†ã«ã¯ã€Œæ­£è¦åŒ–å¾Œã®æ–‡å­—åˆ—ã€ã§æŠ•ç¥¨ã™ã‚‹ã®ãŒè‰¯ã„ãŒã€ã“ã“ã§ã¯ç°¡æ˜“ç‰ˆ
                vote = Counter(valid_answers_for_voting).most_common(1)[0][0]
                if check_correctness(vote, gold):
                    maj_correct_count += 1
            
            # --- 3. Delta-PRM (Best-of-N) ---
            # ã‚¹ã‚³ã‚¢ãŒæœ€å¤§ã®ãƒ‘ã‚¹ã‚’é¸æŠ
            best_idx = np.argmax(scores)
            best_ans = extracted_answers[best_idx]
            
            if check_correctness(best_ans, gold):
                prm_correct_count += 1

        # çµæœé›†è¨ˆ
        pass1_acc = pass1_total_correct / total_generated_paths
        maj_acc = maj_correct_count / total_problems
        prm_acc = prm_correct_count / total_problems
        
        print("\n" + "="*40)
        print(f"EVALUATION RESULTS on {DATASET_NAME} (N={N_SAMPLES})")
        print("="*40)
        print(f"1. Pass@1 (Avg) : {pass1_acc:.2%} (Model's raw capability)")
        print(f"2. Majority Vote: {maj_acc:.2%} (Consensus baseline)")
        print(f"3. Delta-PRM    : {prm_acc:.2%} (Ours)")
        print("="*40)
        
        # å‹åˆ©åˆ¤å®š
        if prm_acc > maj_acc:
            print("ğŸ† Delta-PRM outperforms Majority Voting!")
        elif prm_acc > pass1_acc:
            print("âœ… Delta-PRM improves over Pass@1 (but lost to Voting)")
        else:
            print("âš ï¸ Delta-PRM needs improvement.")

def main():
    evaluator = Evaluator()
    
    # 1. ç”Ÿæˆ
    results = evaluator.run_generation()
    
    # 2. æ¡ç‚¹
    scored_results = evaluator.run_scoring(results)
    
    # 3. è©•ä¾¡
    evaluator.calculate_metrics(scored_results)
    
    # çµæœä¿å­˜
    with open("data/math500_results.json", "w") as f:
        json.dump(scored_results, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    main()